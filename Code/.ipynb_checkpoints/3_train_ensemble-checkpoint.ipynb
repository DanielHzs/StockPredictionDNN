{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cf0603d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T10:11:05.479541Z",
     "start_time": "2025-05-06T10:11:01.801209Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 06:11:02.090570: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746526262.108156 1678293 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746526262.114092 1678293 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746526262.128410 1678293 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746526262.128424 1678293 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746526262.128426 1678293 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746526262.128428 1678293 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-06 06:11:02.133160: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Process SpawnProcess-1:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnProcess-2:\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'train_single' on <module '__main__' (built-in)>\n",
      "Process SpawnProcess-3:\n",
      "Process SpawnProcess-4:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process SpawnProcess-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'train_single' on <module '__main__' (built-in)>\n",
      "Process SpawnProcess-6:\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'train_single' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "Process SpawnProcess-7:\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'train_single' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'train_single' on <module '__main__' (built-in)>\n",
      "Process SpawnProcess-8:\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'train_single' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "Process SpawnProcess-10:\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'train_single' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "Process SpawnProcess-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'train_single' on <module '__main__' (built-in)>\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/home/zh283/.conda/envs/tf_env/lib/python3.11/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'train_single' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 278\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHistories saved for each worker.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 278\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 254\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mworkers) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    253\u001b[0m     futures \u001b[38;5;241m=\u001b[39m [ex\u001b[38;5;241m.\u001b[39msubmit(train_single, i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(workers)]\n\u001b[0;32m--> 254\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# wait & collect\u001b[39;00m\n\u001b[1;32m    256\u001b[0m model_paths, histories \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n\u001b[1;32m    257\u001b[0m ensemble \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model_paths]\n",
      "Cell \u001b[0;32mIn[1], line 254\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mworkers) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    253\u001b[0m     futures \u001b[38;5;241m=\u001b[39m [ex\u001b[38;5;241m.\u001b[39msubmit(train_single, i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(workers)]\n\u001b[0;32m--> 254\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m futures]  \u001b[38;5;66;03m# wait & collect\u001b[39;00m\n\u001b[1;32m    256\u001b[0m model_paths, histories \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n\u001b[1;32m    257\u001b[0m ensemble \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model_paths]\n",
      "File \u001b[0;32m~/.conda/envs/tf_env/lib/python3.11/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/.conda/envs/tf_env/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing as mp\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# Configuration\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "CONFIG = {\n",
    "    'data_dir':\n",
    "    os.environ.get('DATA_DIR', '/gpfs/home/zh283/StockPredictionDNN/Data'),\n",
    "    'variant':\n",
    "    'pct',  # options: raw, pct, z, invn\n",
    "    'label':\n",
    "    'ret_invn',  # options: ret_exc_lead1m, ret_pct, ret_z, ret_invn\n",
    "    'weight':\n",
    "    'w_ew',  # options: w_ew, w_vw\n",
    "    'feature_dim':\n",
    "    153,\n",
    "\n",
    "    # Year splits (inclusive)\n",
    "    'train_years':\n",
    "    list(range(1971, 1997)),\n",
    "    'val_years':\n",
    "    list(range(1998, 2002)),\n",
    "    'predict_years':\n",
    "    list(range(2003, 2024)),\n",
    "\n",
    "    # Model hyperparameters\n",
    "    'num_layers':\n",
    "    5,\n",
    "    'neurons_per_layer':\n",
    "    64,\n",
    "    'dropout':\n",
    "    0.1,\n",
    "    'use_batch_norm':\n",
    "    True,\n",
    "    'loss':\n",
    "    'mse',  # options: mse, mae, huber\n",
    "    'huber_delta':\n",
    "    1.0,\n",
    "    'learning_rate':\n",
    "    1e-3,\n",
    "    'lr_scheduler':\n",
    "    'reduce_on_plateau',  # options: none, exponential_decay, reduce_on_plateau\n",
    "    'decay_steps':\n",
    "    10000,\n",
    "    'decay_rate':\n",
    "    0.96,\n",
    "    'reduce_patience':\n",
    "    5,\n",
    "    'reduce_factor':\n",
    "    0.5,\n",
    "\n",
    "    # Training parameters\n",
    "    'train_batch_size':\n",
    "    1024,\n",
    "    'eval_batch_size':\n",
    "    32976,\n",
    "    'epochs':\n",
    "    50,\n",
    "    'ensemble_size':\n",
    "    10,\n",
    "}\n",
    "CONFIG['tfrecord_dir'] = os.path.join(CONFIG['data_dir'], 'tfrecords',\n",
    "                                      CONFIG['variant'])\n",
    "\n",
    "os.makedirs(CONFIG.get('model_dir', './chkpts_script'), exist_ok=True)\n",
    "\n",
    "# Force safe start‑method once, **before** TF sees CUDA\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# GPU setup\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "PHYSICAL_GPUS = tf.config.list_physical_devices('GPU')\n",
    "# Enable memory growth so each process only allocates as needed\n",
    "for gpu in PHYSICAL_GPUS:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "NUM_GPUS = len(PHYSICAL_GPUS)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# Dataset utilities\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "def parse_example_fn(feature_dim, mode='train'):\n",
    "    desc = {\n",
    "        'feat': tf.io.FixedLenFeature([feature_dim], tf.float32),\n",
    "        CONFIG['label']: tf.io.FixedLenFeature([], tf.float32),\n",
    "        CONFIG['weight']: tf.io.FixedLenFeature([], tf.float32),\n",
    "    }\n",
    "    if mode == 'predict':\n",
    "        desc.update({\n",
    "            'permno': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'eom': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'me': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'size_grp': tf.io.FixedLenFeature([], tf.string),\n",
    "            'crsp_exchcd': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'ret': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'ret_exc': tf.io.FixedLenFeature([], tf.float32),\n",
    "        })\n",
    "\n",
    "    def _parse(record):\n",
    "        ex = tf.io.parse_single_example(record, desc)\n",
    "        features = ex.pop('feat')\n",
    "        label = ex.pop(CONFIG['label'])\n",
    "        weight = ex.pop(CONFIG['weight'])\n",
    "        if mode == 'predict':\n",
    "            return features, label, weight, ex\n",
    "        return features, label, weight\n",
    "\n",
    "    return _parse\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# Dataset creation\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "def make_dataset(years, mode='train'):\n",
    "    # Build list of matching TFRecord files\n",
    "    pattern = os.path.join(CONFIG['tfrecord_dir'], CONFIG['variant'],\n",
    "                           f\"{CONFIG['variant']}-year*.tfrecord\")\n",
    "    files = sorted(\n",
    "        f for f in glob.glob(pattern)\n",
    "        if int(os.path.basename(f).split('year')[1].split('.')[0]) in years)\n",
    "\n",
    "    # Use from_generator to avoid early tf.constant (prevents CUDA init errors)\n",
    "    files_ds = tf.data.Dataset.from_generator(lambda: files,\n",
    "                                              output_types=tf.string,\n",
    "                                              output_shapes=())\n",
    "\n",
    "    if mode == 'train' and len(files) > 1:\n",
    "        files_ds = files_ds.shuffle(len(files))\n",
    "        cycle_length = min(16, len(files))\n",
    "    else:\n",
    "        cycle_length = 1\n",
    "\n",
    "    # Parallel interleave reading TFRecords\n",
    "    ds = files_ds.interleave(tf.data.TFRecordDataset,\n",
    "                             cycle_length=cycle_length,\n",
    "                             num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    parse_fn = parse_example_fn(CONFIG['feature_dim'], mode)\n",
    "    ds = ds.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.cache()\n",
    "    if mode == 'train':\n",
    "        ds = ds.shuffle(10000)\n",
    "\n",
    "    batch_size = CONFIG['train_batch_size'] if mode == 'train' else CONFIG[\n",
    "        'eval_batch_size']\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# Model definition\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "LOSS_MAP = {\n",
    "    'mse': tf.keras.losses.MeanSquaredError(),\n",
    "    'mae': tf.keras.losses.MeanAbsoluteError(),\n",
    "    'huber': tf.keras.losses.Huber(delta=CONFIG['huber_delta']),\n",
    "}\n",
    "LR_SCHEDULES = {\n",
    "    'none':\n",
    "    lambda: CONFIG['learning_rate'],\n",
    "    'exponential_decay':\n",
    "    lambda: tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=CONFIG['learning_rate'],\n",
    "        decay_steps=CONFIG['decay_steps'],\n",
    "        decay_rate=CONFIG['decay_rate'],\n",
    "        staircase=True),\n",
    "}\n",
    "\n",
    "\n",
    "def train_single(idx: int):\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        tf.config.set_visible_devices(gpus[idx % len(gpus)], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[idx % len(gpus)], True)\n",
    "\n",
    "    train_ds = make_dataset(CONFIG['train_years'], 'train')\n",
    "    val_ds = make_dataset(CONFIG['val_years'], 'val')\n",
    "\n",
    "    mdl = build_model()\n",
    "    cbs = [\n",
    "        tf.keras.callbacks.EarlyStopping('val_loss',\n",
    "                                         patience=5,\n",
    "                                         restore_best_weights=True)\n",
    "    ]\n",
    "    if CONFIG['lr_scheduler'] == 'reduce_on_plateau':\n",
    "        cbs.append(\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                patience=CONFIG['reduce_patience'],\n",
    "                factor=CONFIG['reduce_factor'],\n",
    "                verbose=1))\n",
    "\n",
    "    hist = mdl.fit(train_ds,\n",
    "                   validation_data=val_ds,\n",
    "                   epochs=CONFIG['epochs'],\n",
    "                   callbacks=cbs,\n",
    "                   verbose=2)\n",
    "\n",
    "    path = os.path.join(CONFIG['model_dir'], f'ensemble_{idx}.keras')\n",
    "    mdl.save(path, save_format='keras_v3')\n",
    "    return path, hist.history\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# Single-model training function (spawn-safe)\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "def train_single(idx: int):\n",
    "    # isolate one GPU\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        tf.config.set_visible_devices(gpus[idx % len(gpus)], 'GPU')\n",
    "        for g in gpus:\n",
    "            tf.config.experimental.set_memory_growth(g, True)\n",
    "\n",
    "    train_ds = make_dataset(CONFIG['train_years'], 'train')\n",
    "    val_ds = make_dataset(CONFIG['val_years'], 'val')\n",
    "\n",
    "    model = build_model()\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping('val_loss',\n",
    "                                         patience=5,\n",
    "                                         restore_best_weights=True)\n",
    "    ]\n",
    "    if CONFIG['lr_scheduler'] == 'reduce_on_plateau':\n",
    "        callbacks.append(make_lr_callback())\n",
    "\n",
    "    hist = model.fit(train_ds,\n",
    "                     validation_data=val_ds,\n",
    "                     epochs=CONFIG['epochs'],\n",
    "                     verbose=2,\n",
    "                     callbacks=callbacks)\n",
    "\n",
    "    mdl_path = os.path.join(CONFIG['model_dir'], f'ensemble_{idx}.keras')\n",
    "    model.save(mdl_path, save_format='keras_v3')  # light, TF‑native format\n",
    "    return mdl_path, hist.history  # both are trivially picklable\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# 5. Main\n",
    "# --------------------------------------------------------------------------- #\n",
    "def main():\n",
    "    workers = CONFIG['ensemble_size']\n",
    "    with ProcessPoolExecutor(max_workers=workers) as ex:\n",
    "        futures = [ex.submit(train_single, i) for i in range(workers)]\n",
    "        results = [f.result() for f in futures]  # wait & collect\n",
    "\n",
    "    model_paths, histories = zip(*results)\n",
    "    ensemble = [tf.keras.models.load_model(p) for p in model_paths]\n",
    "\n",
    "    pred_ds = make_dataset(CONFIG['predict_years'], 'predict')\n",
    "    rows = []\n",
    "    for feats, _, _, meta in pred_ds:\n",
    "        preds = tf.reduce_mean(tf.stack(\n",
    "            [m(feats, training=False) for m in ensemble], axis=0),\n",
    "                               axis=0).numpy().ravel()\n",
    "        meta_np = {k: v.numpy() for k, v in meta.items()}\n",
    "        for i, p in enumerate(preds):\n",
    "            rec = {k: meta_np[k][i] for k in meta_np}\n",
    "            rec['prediction'] = float(p)\n",
    "            rows.append(rec)\n",
    "\n",
    "    out_df = pd.DataFrame(rows)\n",
    "    out_df.to_parquet('ensemble_predictions.parquet', index=False)\n",
    "    print(out_df.head())\n",
    "    print(\"\\nHistories saved for each worker.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d48923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
